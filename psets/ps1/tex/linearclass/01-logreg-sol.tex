\begin{answer}
Compute the gradient of the cost function $J(\theta)$, we have:
%
\begin{align}
	\nabla_{\theta}J(\theta) 
	&= \frac{1}{n} \sum \limits_{i = 1}^{n} \left(-y^{(i)} \nabla_{\theta}\log(g(\theta^{T}\xsi)) - (1 - y^{(i)}) \nabla_{\theta}\log(1 - g(\theta^{T}\xsi))\right) \\
	&= \frac{1}{n} \sum_{i = 1}^{n} \left(-y^{(i)}(1 - g(\theta^{T}\xsi))\xsi + (1 - y^{(i)}) g(\theta^{T}\xsi) x^{(i)}\right) \\
	&= \frac{1}{n} \sum \limits_{i = 1}^{n} (g(\theta^{T}\xsi) - y^{(i)}) x^{(i)}
\end{align}
%
Now, take the second derivative of the gradient, we obtain the hessian matrix:
%
\begin{align}
	H(J)(\theta) 
	= \frac{1}{n} \sum \limits_{i = 1}^{n} g(\theta^{T}\xsi)(1 - g(\theta^{T}\xsi)) x^{(i)} {x^{(i)}}^T
\end{align}
%
Remember that $h_{\theta}(x^{(i)}) = g(\theta^{T}\xsi)$.
For any vector $z \in \mathbb{R}^{d + 1}$, let's consider:
%
\begin{align}
	z^{T} H(J)(\theta) z
	&= \frac{1}{n} \sum \limits_{i = 1}^{n} h_{\theta}(x^{(i)})(1 - h_{\theta}(x^{(i)})) (z^{T} x^{(i)})^2
\end{align}
%
All the terms are non-negative as $h_{\theta}(x^{(i)})$ is a probability and squares are non-negative. This implies that $z^{T} H(J)(\theta) z \ge 0$ for all vector $z \in \mathbb{R}^{d + 1}$, hence $H(J)(\theta) \succeq 0$. \\
\end{answer}




















